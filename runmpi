export OMP_NUM_THREADS=${OMP_NUM_THREADS:-1}
[ -z "$nprocs" ] && echo "Need to set nprocs" && exit 1;
[ -z "$machine" ] && echo "Need to set machine" && exit 1;
[ -z "$PGM" ] && echo "Need to set PGM" && exit 1;
if [ "$machine" == 'wcoss' ] ; then
   [ -z "$mpitaskspernode" ] && echo "Need to set mpitaskspernode" && exit 1;
   echo "running aprun -n $nprocs -N $mpitaskspernode -d ${OMP_NUM_THREADS} --cc depth $PGM"
   eval aprun -n $nprocs -N $mpitaskspernode -d ${OMP_NUM_THREADS} --cc depth $PGM
elif [ "$machine" == 'gaea' ] ; then
   [ -z "$mpitaskspernode" ] && echo "Need to set mpitaskspernode" && exit 1;
   if [ -z $SLURM_JOB_ID ]; then
     # use aprun
     echo "running aprun -n $nprocs -N $mpitaskspernode -d ${OMP_NUM_THREADS} --cc depth $PGM"
     eval aprun -n $nprocs -N $mpitaskspernode -d ${OMP_NUM_THREADS} --cc depth $PGM
   else
     # use srun
     export OMP_PROC_BIND=spread
     export OMP_PLACES=threads
     totcores=`expr $nprocs \* $OMP_NUM_THREADS`
     totnodes=`python -c "import math; print int(math.ceil(float(${totcores})/${corespernode}))"`
     count=`python -c "import math; print int(math.floor(float(${corespernode})/${mpitaskspernode}))"` 
     # -c: cpus per task (number of threads per mpi task)
     # -n: number of mpi tasks
     # -N: number of nodes to run on
     if [ -z $SRUN_TIME_LIMIT ]; then
       echo "running srun -N $totnodes -n $nprocs -c ${count} --cpu_bind=cores $PGM"
       eval srun -N $totnodes -n $nprocs -c ${count} --cpu_bind=cores $PGM
     else
       echo "running srun -N $totnodes -n $nprocs -c ${count} --cpu_bind=cores --time=$SRUN_TIME_LIMIT $PGM"
       eval srun -N $totnodes -n $nprocs -c ${count} --cpu_bind=cores --time=$SRUN_TIME_LIMIT $PGM
     fi
     rc=$?
   fi
elif [ "$machine" == 'theia' ]; then
   if [ -z $SLURM_JOB_ID ]; then
      # HOSTFILE env var must be set
      [ -z "$HOSTFILE" ] && echo "Need to set HOSTFILE" && exit 1;
      cat $HOSTFILE
      echo "running mpirun -np $nprocs -machinefile $HOSTFILE $PGM"
      echo "OMP_NUM_THREADS = $OMP_NUM_THREADS"
      eval mpirun -np $nprocs -machinefile $HOSTFILE $PGM
      rc=$?
   else
     # use srun
     export OMP_PROC_BIND=spread
     export OMP_PLACES=threads
     totcores=`expr $nprocs \* $OMP_NUM_THREADS`
     totnodes=`python -c "import math; print int(math.ceil(float(${totcores})/${corespernode}))"`
     count=`python -c "import math; print int(math.floor(float(${corespernode})/${mpitaskspernode}))"` 
     # -c: cpus per task (number of threads per mpi task)
     # -n: number of mpi tasks
     # -N: number of nodes to run on
     if [ -z $SRUN_TIME_LIMIT ]; then
       echo "running srun -N $totnodes -n $nprocs -c ${count} --cpu_bind=cores $PGM"
       eval srun -N $totnodes -n $nprocs -c ${count} --cpu_bind=cores $PGM
     else
       echo "running srun -N $totnodes -n $nprocs -c ${count} --cpu_bind=cores --time=$SRUN_TIME_LIMIT $PGM"
       eval srun -N $totnodes -n $nprocs -c ${count} --cpu_bind=cores --time=$SRUN_TIME_LIMIT $PGM
     fi
     rc=$?
   fi
elif [ "$machine" == 'cori' ]; then
   [ -z "$mpitaskspernode" ] && echo "Need to set mpitaskspernode" && exit 1;
   export OMP_PROC_BIND=spread
   export OMP_PLACES=threads
   totcores=`expr $nprocs \* $OMP_NUM_THREADS`
   totnodes=`python -c "import math; print int(math.ceil(float(${totcores})/${corespernode}))"`
   count=`python -c "import math; print int(math.floor(float(${corespernode})/${mpitaskspernode}))"` 
   #count=`python -c "import math; print 2*int(math.floor(float(${corespernode})/${mpitaskspernode}))"`  #hyperthreads
   #srun -N $totnodes -n $nprocs -c ${count} --cpu_bind=cores check-hybrid.intel.cori|sort -nk 4 
   echo "running srun -N $totnodes -n $nprocs -c ${count} --cpu_bind=cores $PGM"
   eval srun -N $totnodes -n $nprocs -c ${count} --cpu_bind=cores $PGM
   rc=$?
else
   echo "machine must be 'wcoss', 'theia', 'cori' or 'jet', got $machine"
   rc=1
fi
echo "exiting runmpi..."
exit $rc
